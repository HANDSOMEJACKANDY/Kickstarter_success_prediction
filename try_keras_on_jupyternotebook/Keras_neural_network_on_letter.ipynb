{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a document that tries keras on jupyter notebook\n",
    "# I attempted to use letter by letter plus RNN to inteprete description, but it seems to be not good\n",
    "# possible reasons including too small database, too difficult the task.\n",
    "# I am starting another atempt through word base classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:57:46.610487Z",
     "start_time": "2017-11-13T20:57:46.589130Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback, LambdaCallback, TensorBoard, ReduceLROnPlateau\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T19:10:52.320786Z",
     "start_time": "2017-11-13T19:10:52.233221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_desired_letter(char):\n",
    "    return ord(char) >= 97 and ord(char) < 123 or ord(char) >= 48 and ord(char) < 58 or ord(char) == ord(\".\") or ord(char) == ord(\",\") or ord(char) == ord(\" \")\n",
    "\n",
    "\n",
    "def get_train_data(train_portion):\n",
    "    # load the dataset but only keep the top n words, zero the rest\n",
    "    train_data = pd.read_csv(\"input/kickstarter_train.csv\")\n",
    "    train_texts_and_results = train_data.iloc[:, [2, -1]]\n",
    "    # get split point for train and test data\n",
    "    split_point = int(train_portion * len(train_data))\n",
    "    # do preliminary preprocessing:remove all symbols\n",
    "    train_data[\"desc\"] = [[char for char in str(text).lower() if is_desired_letter(char)] for\n",
    "                          text in train_data[\"desc\"]]\n",
    "    # remove too short desc\n",
    "    drop_index = []\n",
    "    for i in range(len(train_data)):\n",
    "        if len(train_data.iloc[i, 2]) <= 20:\n",
    "            drop_index.append(i)\n",
    "    train_data.drop(train_data.index[drop_index])\n",
    "    # get descriptions data\n",
    "    train_texts = np.array(train_data.iloc[:split_point, 2])\n",
    "    test_texts = np.array(train_data.iloc[split_point:, 2])\n",
    "    # get num data\n",
    "    train_num = np.array(train_data.iloc[:split_point, [3, 12]])\n",
    "    test_num = np.array(train_data.iloc[split_point:, [3, 12]])\n",
    "    # get result data\n",
    "    train_results = np.array(train_data.iloc[:split_point, -1])\n",
    "    test_results = np.array(train_data.iloc[split_point:, -1])\n",
    "    \n",
    "    return train_texts, train_num, train_results, test_texts, test_num, test_results\n",
    "\n",
    "\n",
    "def convert_to_onehot(data, num_features):\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        new_data.append(np_utils.to_categorical(item, num_classes=num_features))\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T19:11:17.010798Z",
     "start_time": "2017-11-13T19:10:58.252705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data grabbed\n",
      "tokenizing and normalizing is done\n"
     ]
    }
   ],
   "source": [
    "# get training testing data from disk\n",
    "train_data_portion = 0.95\n",
    "trainX_desc, trainX_num, trainY, testX_desc, testX_num, testY = get_train_data(train_data_portion)\n",
    "print(\"data grabbed\")\n",
    "\n",
    "# convert char to int, and \n",
    "all_letters = sorted(set([char for text in trainX_desc for char in text]))\n",
    "n_vacab = len(all_letters)\n",
    "char_to_int = dict((c, float(i+1)) for i, c in enumerate(all_letters))\n",
    "trainX_desc = [[char_to_int[char] * 2 / float(n_vacab) - 1 for char in text] for text in trainX_desc]\n",
    "testX_desc = [[char_to_int[char] * 2 / float(n_vacab) - 1 for char in text] for text in testX_desc]\n",
    "print(\"tokenizing and normalizing is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T19:11:57.283753Z",
     "start_time": "2017-11-13T19:11:56.049499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding finished\n",
      "reshaping data with shape (86503, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing description data\n",
    "# truncate and pad input sequences\n",
    "max_desc_length = 200\n",
    "trainX_desc = sequence.pad_sequences(list(trainX_desc), maxlen=max_desc_length, truncating=\"post\")\n",
    "testX_desc = sequence.pad_sequences(list(testX_desc), maxlen=max_desc_length, truncating=\"post\")\n",
    "print(\"padding finished\")\n",
    "\n",
    "# reshape trainX to multi_timestep single feature\n",
    "time_steps = max_desc_length\n",
    "num_features = 1\n",
    "testX_desc = np.array(testX_desc)\n",
    "testX_desc = testX_desc.reshape((-1, time_steps, num_features))\n",
    "trainX_desc = np.array(trainX_desc)\n",
    "trainX_desc = trainX_desc.reshape((-1, time_steps, num_features))\n",
    "print(\"reshaping data with shape {}\".format(trainX_desc.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T19:48:53.270847Z",
     "start_time": "2017-11-13T19:48:53.263918Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class My_Callback(Callback):\n",
    "    def on_epoch_begin(self, logs={}):\n",
    "        scores = self.model.evaluate(self.model.validation_data[0], self.model.validation_data[1], verbose=1)\n",
    "        print(\"Accuracy on Validation data:{}\".format(np.array(scores).mean()))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T21:17:57.270910Z",
     "start_time": "2017-11-13T20:57:50.237610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 200, 64)           16896     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 54,401\n",
      "Trainable params: 54,273\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "model building finished\n",
      " None\n",
      "Train on 86503 samples, validate on 21626 samples\n",
      "Epoch 1/10\n",
      "86503/86503 [==============================] - 148s 2ms/step - loss: 0.6466 - acc: 0.6563 - val_loss: 0.5549 - val_acc: 0.7732\n",
      "Epoch 2/10\n",
      "86503/86503 [==============================] - 151s 2ms/step - loss: 0.6443 - acc: 0.6572 - val_loss: 0.5714 - val_acc: 0.7732\n",
      "Epoch 3/10\n",
      "86503/86503 [==============================] - 147s 2ms/step - loss: 0.6439 - acc: 0.6572 - val_loss: 0.5521 - val_acc: 0.7732\n",
      "Epoch 4/10\n",
      "86503/86503 [==============================] - 143s 2ms/step - loss: 0.6441 - acc: 0.6572 - val_loss: 0.5614 - val_acc: 0.7732\n",
      "Epoch 5/10\n",
      "86503/86503 [==============================] - 142s 2ms/step - loss: 0.6436 - acc: 0.6572 - val_loss: 0.5570 - val_acc: 0.7732\n",
      "Epoch 6/10\n",
      "86503/86503 [==============================] - 144s 2ms/step - loss: 0.6436 - acc: 0.6572 - val_loss: 0.5550 - val_acc: 0.7732\n",
      "Epoch 7/10\n",
      "86503/86503 [==============================] - 143s 2ms/step - loss: 0.6434 - acc: 0.6572 - val_loss: 0.5471 - val_acc: 0.7732\n",
      "Epoch 8/10\n",
      "79616/86503 [==========================>...] - ETA: 10s - loss: 0.6444 - acc: 0.6563"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bad141ccf2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mvalidation_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMy_Callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m description_model.fit(trainX_desc, trainY, epochs=10, batch_size=256, shuffle=True, validation_data=[testX_desc, testY], \n\u001b[0;32m---> 19\u001b[0;31m                       verbose=1, callbacks=[TensorBoard(), ReduceLROnPlateau()])\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# see actual result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate model for descriptions\n",
    "description_model = Sequential()\n",
    "description_model.add(LSTM(64, input_shape=(time_steps, num_features), return_sequences=True))\n",
    "description_model.add(LSTM(64))\n",
    "description_model.add(Dropout(0.2))\n",
    "description_model.add(Dense(units=64, activation=\"sigmoid\"))\n",
    "description_model.add(BatchNormalization())\n",
    "description_model.add(Dropout(0.5))\n",
    "description_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# configurate model training\n",
    "description_model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(\"model building finished\\n\", description_model.summary())\n",
    "\n",
    "\n",
    "# do training\n",
    "validation_callback = My_Callback()\n",
    "description_model.fit(trainX_desc, trainY, epochs=10, batch_size=256, shuffle=True, validation_data=[testX_desc, testY], \n",
    "                      verbose=1, callbacks=[TensorBoard(), ReduceLROnPlateau()])\n",
    "\n",
    "# see actual result\n",
    "scores = description_model.evaluate(testX_desc, testY, verbose=1)\n",
    "print(\"Accuracy:{}\".format(np.array(scores).mean()))\n",
    "\n",
    "# save model\n",
    "filepath = \"description_model_weights.h5\"\n",
    "description_model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T17:52:26.288736Z",
     "start_time": "2017-11-13T17:47:32.081434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86503 samples, validate on 21626 samples\n",
      "Epoch 1/10\n",
      " - 28s - loss: 0.6431 - acc: 0.6571 - val_loss: 0.5618 - val_acc: 0.7732\n",
      "Epoch 2/10\n",
      " - 28s - loss: 0.6431 - acc: 0.6570 - val_loss: 0.5801 - val_acc: 0.7732\n",
      "Epoch 3/10\n",
      " - 29s - loss: 0.6431 - acc: 0.6571 - val_loss: 0.5705 - val_acc: 0.7732\n",
      "Epoch 4/10\n",
      " - 28s - loss: 0.6430 - acc: 0.6571 - val_loss: 0.5586 - val_acc: 0.7732\n",
      "Epoch 5/10\n",
      " - 29s - loss: 0.6431 - acc: 0.6572 - val_loss: 0.5793 - val_acc: 0.7732\n",
      "Epoch 6/10\n",
      " - 30s - loss: 0.6430 - acc: 0.6572 - val_loss: 0.5500 - val_acc: 0.7732\n",
      "Epoch 7/10\n",
      " - 28s - loss: 0.6431 - acc: 0.6571 - val_loss: 0.5464 - val_acc: 0.7732\n",
      "Epoch 8/10\n",
      " - 28s - loss: 0.6430 - acc: 0.6571 - val_loss: 0.5721 - val_acc: 0.7732\n",
      "Epoch 9/10\n",
      " - 31s - loss: 0.6431 - acc: 0.6572 - val_loss: 0.5711 - val_acc: 0.7732\n",
      "Epoch 10/10\n",
      " - 28s - loss: 0.6430 - acc: 0.6572 - val_loss: 0.5671 - val_acc: 0.7732\n",
      "21626/21626 [==============================] - 8s 367us/step\n",
      "Accuracy:0.6701744736208347\n"
     ]
    }
   ],
   "source": [
    "description_model.load_weights(filepath)\n",
    "\n",
    "# do training\n",
    "description_model.fit(trainX_desc, trainY, epochs=10, batch_size=256, shuffle=True, validation_data=[testX_desc, testY], verbose=1)\n",
    "\n",
    "# see original result\n",
    "scores = description_model.evaluate(testX_desc, testY, verbose=1)\n",
    "print(\"Accuracy:{}\".format(np.array(scores).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate model for num data\n",
    "num_model = Sequential()\n",
    "num_model.add(Dense(units=64, input_shape=(2)))\n",
    "num_model.add(BatchNormalization())\n",
    "num_model.add(Activation(\"sigmoid\"))\n",
    "num_model.add(Dropout(0.2))\n",
    "num_model.add(Dense(units=64))\n",
    "num_model.add(BatchNormalization())\n",
    "num_model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 278,
   "position": {
    "height": "40px",
    "left": "1125px",
    "right": "78px",
    "top": "189px",
    "width": "477px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
