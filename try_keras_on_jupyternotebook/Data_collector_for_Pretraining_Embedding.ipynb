{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T10:27:27.699906Z",
     "start_time": "2017-11-16T10:27:23.713776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import GaussianNoise, BatchNormalization, Dropout\n",
    "from keras.layers import Activation, Input, concatenate, Reshape, merge, dot\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.callbacks import Callback, LambdaCallback, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from random import shuffle\n",
    "import time\n",
    "import pylab as pl\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T10:27:27.801322Z",
     "start_time": "2017-11-16T10:27:27.795838Z"
    }
   },
   "outputs": [],
   "source": [
    "# set backends of keras\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T10:27:27.848768Z",
     "start_time": "2017-11-16T10:27:27.825228Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_desired_letter(char):\n",
    "    order = ord(char)\n",
    "    return order >= 97 and order < 123 or order >= 48 and order < 58 or order == ord(\" \") or order == ord(\"'\")\n",
    "\n",
    "\n",
    "def get_text_data():\n",
    "    # load the dataset but only keep the top n words, zero the rest\n",
    "    train_data = pd.read_csv(\"input/kickstarter_train.csv\")\n",
    "    # segment all sentences\n",
    "    sent_list = [sent.lower() for text in train_data[\"desc\"] if type(text) is str for sent in sent_tokenize(text)]\n",
    "    # remove symbols in each description\n",
    "    sent_list = [[char for char in sent if is_desired_letter(char)] for sent in sent_list]\n",
    "    sent_list = [''.join(sent).split() for sent in sent_list]\n",
    "    # remove too short desc\n",
    "    train_texts = [sent for sent in sent_list if len(sent) > 3]\n",
    "\n",
    "    return train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T10:27:50.032150Z",
     "start_time": "2017-11-16T10:27:30.717683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data grabbed\n"
     ]
    }
   ],
   "source": [
    "# get training texts from disk\n",
    "train_texts = get_text_data()\n",
    "print(\"data grabbed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation boosted the result\n",
    "### Common terms make phrases more reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T10:28:06.503405Z",
     "start_time": "2017-11-16T10:28:00.891684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram vocabulary size:  15941\n"
     ]
    }
   ],
   "source": [
    "# train bigram phrases\n",
    "# min_count 15, threshold 0.8, scorer 'npmi', max_vocab_size 50000 seems great\n",
    "common_words = get_stop_words('en')\n",
    "common_words.extend([\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"])\n",
    "bigram = Phrases(common_terms=common_words, sentences=train_texts, scoring='npmi', min_count=20, threshold=0.8, max_vocab_size=40000)\n",
    "print(\"bigram vocabulary size: \", len(bigram.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T10:30:53.886259Z",
     "start_time": "2017-11-16T10:30:53.849101Z"
    }
   },
   "outputs": [],
   "source": [
    "# save bigram\n",
    "bigram.save(\"my_bigram_with_vocab_{}.pkl\".format(len(bigram.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T02:55:43.418112Z",
     "start_time": "2017-11-16T02:55:29.049908Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andywu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/gensim/models/phrases.py:431: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dictionary length is:  13151\n",
      "dictionary vocabulary adding finished, now start non_sense rotation...\n"
     ]
    }
   ],
   "source": [
    "# get dictionary of word with fair frequency\n",
    "no_below = 8\n",
    "non_sense = \"9898989898i98989i89\"\n",
    "dictionary = Dictionary(documents=bigram[train_texts])\n",
    "dictionary.filter_extremes(no_below=no_below)\n",
    "dictionary.compactify()\n",
    "print(\"current dictionary length is: \", len(dictionary))\n",
    "print(\"dictionary vocabulary adding finished, now start non_sense rotation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T02:55:44.139415Z",
     "start_time": "2017-11-16T02:55:43.903989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary vocabulary length: 13152\n"
     ]
    }
   ],
   "source": [
    "# rotate the dictionary until non-sense become index 0, preparing for future padding and make sample table\n",
    "dictionary.add_documents([[non_sense]])\n",
    "index_dfs_list = [(dictionary[i], dictionary.dfs[i]) for i in range(len(dictionary) - 1)]\n",
    "index_dfs_list = sorted(index_dfs_list, key=lambda x: -x[1])\n",
    "dictionary.filter_tokens(good_ids=[len(dictionary)-1]) # preserve only non_sense\n",
    "# add vocabs back in sequence\n",
    "for i in range(len(index_dfs_list)):\n",
    "    dictionary.add_documents([[index_dfs_list[i][0]]])\n",
    "    dictionary.dfs[i+1] = index_dfs_list[i][1]\n",
    "\n",
    "n_vocab = len(dictionary)\n",
    "print(\"dictionary vocabulary length:\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T02:55:44.681302Z",
     "start_time": "2017-11-16T02:55:44.667350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary saved\n"
     ]
    }
   ],
   "source": [
    "dictionary.save(\"dictionary_least_dfs_{}_vocab_{}.pkl\".format(no_below, n_vocab))\n",
    "print(\"dictionary saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T02:55:55.235296Z",
     "start_time": "2017-11-16T02:55:45.230910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andywu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/gensim/models/phrases.py:431: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing is done\n",
      "left texts num is:  133686\n",
      "The max len is 34, the average len is 13.97717786454827, the min len is 6\n"
     ]
    }
   ],
   "source": [
    "# tokenize the texts and remove too short texts\n",
    "train_texts = bigram[train_texts]\n",
    "train_texts = [[dictionary.token2id[word] for word in text if word in dictionary.token2id] for text in train_texts]\n",
    "train_texts = [text for text in train_texts if len(text) > 5]\n",
    "print(\"tokenizing is done\")\n",
    "print(\"left texts num is: \", len(train_texts))\n",
    "t = np.array([len(text) for text in train_texts])\n",
    "print(\"The max len is {}, the average len is {}, the min len is {}\".format(t.max(), t.mean(), t.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T02:59:52.353512Z",
     "start_time": "2017-11-16T02:57:01.942991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ac3435ab3e4ac09aae10ed1bb64c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing training data is done\n"
     ]
    }
   ],
   "source": [
    "# generate and save training data\n",
    "# generate progress bar\n",
    "f = FloatProgress(min=0, max=100)\n",
    "display.display(f)\n",
    "# load training data\n",
    "training_targets = []\n",
    "training_contexts = []\n",
    "training_pairs = []\n",
    "training_labels = []\n",
    "shuffle(train_texts)\n",
    "for i, text in enumerate(train_texts):\n",
    "    pairs, labels = skipgrams(sampling_table=make_sampling_table(n_vocab), sequence=text, vocabulary_size=n_vocab, \n",
    "                                  negative_samples=1., window_size=3)\n",
    "    if not pairs:\n",
    "        continue\n",
    "    pairs = [np.array(x) for x in zip(*pairs)]\n",
    "    labels = np.array(labels)\n",
    "    training_targets.append(pairs[0])\n",
    "    training_contexts.append(pairs[1])\n",
    "    training_labels.append(labels)\n",
    "    f.value = 100 * float(i)/float(len(train_texts))\n",
    "\n",
    "training_pairs = [np.hstack(training_targets), np.hstack(training_contexts)]\n",
    "training_labels = np.hstack(training_labels)\n",
    "# save the loaded file to file\n",
    "np.savez(\"training pairs and labels.npz\", target=training_pairs[0], context=training_pairs[1], labels=training_labels)\n",
    "training_pairs_labels = np.load(\"training pairs and labels.npz\")\n",
    "training_pairs = [training_pairs_labels['target'], training_pairs_labels['context']]\n",
    "training_labels = training_pairs_labels['labels']\n",
    "# print(training_pairs[0].shape)\n",
    "# print(training_pairs[1].shape)\n",
    "# print(training_labels.shape)\n",
    "print(\"preparing training data is done\")\n",
    "print(\"data no is: \", len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-16T03:00:42.830091Z",
     "start_time": "2017-11-16T03:00:42.454000Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savez(\"training pairs and labels.npz\", target=training_pairs[0], context=training_pairs[1], labels=training_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 412,
   "position": {
    "height": "40px",
    "left": "1311px",
    "right": "19px",
    "top": "105px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
